# ============================================================================
# Research Proxy Configuration Template
# ============================================================================
# Copy this file to .env and fill in your actual values
# IMPORTANT: Never commit .env to git (it contains secrets!)
# ============================================================================

# ----------------------------------------------------------------------------
# vLLM Backend Configuration
# ----------------------------------------------------------------------------
VLLM_BASE_URL=http://localhost:8078/v1

# ----------------------------------------------------------------------------
# MCP Server Configuration (mcpragcrawl4ai)
# ----------------------------------------------------------------------------
REST_API_URL=http://localhost:8080/api/v1
REST_API_KEY=your-mcp-api-key-here

# ----------------------------------------------------------------------------
# External API Keys
# ----------------------------------------------------------------------------
# Serper API for web search (https://serper.dev)
SERPER_API_KEY=your-serper-api-key-here

# ----------------------------------------------------------------------------
# Research Queue Limits
# ----------------------------------------------------------------------------
# Maximum concurrent standard research requests (2 iterations)
MAX_STANDARD_RESEARCH=3

# Maximum concurrent deep research requests (4 iterations)
MAX_DEEP_RESEARCH=1

# ----------------------------------------------------------------------------
# Timeout Configuration (seconds)
# ----------------------------------------------------------------------------
# Timeout for vLLM requests
VLLM_TIMEOUT=300

# Timeout for MCP tool calls
MCP_TIMEOUT=60

# Timeout for Serper API calls
SERPER_TIMEOUT=30

# ----------------------------------------------------------------------------
# Server Configuration
# ----------------------------------------------------------------------------
# Host to bind to (0.0.0.0 for all interfaces, 127.0.0.1 for localhost only)
HOST=0.0.0.0

# Port to listen on
PORT=8079

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# ----------------------------------------------------------------------------
# Feature Flags
# ----------------------------------------------------------------------------
# Enable/disable model name auto-detection
AUTO_DETECT_MODEL=true

# Model name polling interval (seconds)
MODEL_POLL_INTERVAL=2

# ----------------------------------------------------------------------------
# Fast Token Estimation (Optimization)
# ----------------------------------------------------------------------------
# Conservative chars/token ratio for quick estimation
CHARS_PER_TOKEN_CONSERVATIVE=1.3

# Threshold to trigger tokenizer (tokens below use fast estimation)
TOKENIZER_THRESHOLD_TOKENS=225000
